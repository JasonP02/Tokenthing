# Dataset
hf_dataset_names: "roneneldan/TinyStories"

# Tokenizer parameters
tokenizer_vocab_size: 30000
tokenizer_sequence_length: 1000
tokenizer_save_path: "./tokenizer_model"